<!doctype html><html lang=zh-cn dir=ltr><head><meta charset=utf-8><meta name=viewport content='width=device-width,initial-scale=1'><meta name=description content="Figure AI Helix 系统总结 硬件组成 感知系统：Figure 02的头部、前躯干和后躯干共配备6个RGB摄像头 决策系统：嵌入式控制板及2块板载低功耗GPUs（NVIDIA RTX GPU，具体型号暂不明确）参考来源 执行系统：人形上身，腰部+头部+双臂共35个关节（从照片推测，头2 + 腰3 + 臂7x2 + 手16x2），部分手部自由度应该没有用到 网络结构 VLA网络由解耦的两个系统——预训练视觉语言模型（S2） + 控制小模型（S1）组成，两者在各自的时间尺度上运行，结合VLM模型广泛通用但不够快的特点和视觉运动策略快速而不广泛的特点，实现一个既广泛又快速的VLA控制模型。\nS2 参数量：7B\n类型：开源VLM\n输入：RGB图片、关节角信息\n输出：潜空间向量\n输出频率：7-9Hz\n作用：场景理解和语义理解，提供跨物体和场景的泛化能力，将所有与语义任务相关的信息提炼到一个单一的连续潜在向量中，并将其传递给S1\nS1 参数量：80M\n类型：交叉注意力的编解码Transformer网络，依赖一个多尺度的全卷积视觉骨干进行视觉处理（在仿真器中预训练初始化*）\n输入：潜空间向量、RGB图片、关节角\n输出：高频机器人动作（关节角）\n输出频率：200Hz\n作用：快速灵巧的控制策略，结合图像编码和当前关节角信息，将潜空间向量表示转化成连续的机器人动作（目标关节角）\n训练细节： 训练数据：约500h的高质量、多机器人、多操作员的多样化遥操作行为数据集\n训练方法：基于raw pixel和文本指令到连续动作的映射，做端到端的训练，采用标准的回归损失。梯度从S1通过潜表示向量传递到S2，实现两者的协同优化。\n注意：\nS1和S2在训练阶段是耦合的 为了模拟真实的推理延迟，在训练阶段认为引入了S1和S2输入量的时间偏移* 调优的流式推理 S1和S2分别运行在一块专门的GPU上，基于共享的内存形成一个生产者-消费者模型。\nS2异步地处理最新的观测信息和自然语言指令， 持续地生成编码了高维意图行为意图的潜向量，并更新到共享内存中。\nS1在独立线程中，结合最新观测和自然语言指令，消费共享内存中的潜向量生成连续的机器人动作。由于S1比S2有更高的推理速度，因此有更高的时间分辨率，从而可以实现更紧密的闭环实时控制。\n数据流与传输带宽 数据流 Sensor以20Hz的频率采集图像和关节角。\nObservation（raw_img） ────传感器图像────>多尺度立体视觉网络 ────合并图像特征────> Observation（combind_img_token）\nObservation（command + state + combind_img_token）────79Hz观测信息────> S2 ────79Hz潜向量────> S1 ────200Hz动作────>执行器\nObservation（command + state + combind_img_token）─────20Hz观测信息────> S1\n"><title>Figure AI Helix System 论文总结</title>
<link rel=canonical href=https://RoboticsChen.github.io/articles/figure-ai-helix-system-paper-reading/><link rel=stylesheet href=/scss/style.min.91ab85d910aadfc4aedff71064d967b220a64d57791d2f588dc44b40735bb8c4.css><meta property='og:title' content="Figure AI Helix System 论文总结"><meta property='og:description' content="Figure AI Helix 系统总结 硬件组成 感知系统：Figure 02的头部、前躯干和后躯干共配备6个RGB摄像头 决策系统：嵌入式控制板及2块板载低功耗GPUs（NVIDIA RTX GPU，具体型号暂不明确）参考来源 执行系统：人形上身，腰部+头部+双臂共35个关节（从照片推测，头2 + 腰3 + 臂7x2 + 手16x2），部分手部自由度应该没有用到 网络结构 VLA网络由解耦的两个系统——预训练视觉语言模型（S2） + 控制小模型（S1）组成，两者在各自的时间尺度上运行，结合VLM模型广泛通用但不够快的特点和视觉运动策略快速而不广泛的特点，实现一个既广泛又快速的VLA控制模型。\nS2 参数量：7B\n类型：开源VLM\n输入：RGB图片、关节角信息\n输出：潜空间向量\n输出频率：7-9Hz\n作用：场景理解和语义理解，提供跨物体和场景的泛化能力，将所有与语义任务相关的信息提炼到一个单一的连续潜在向量中，并将其传递给S1\nS1 参数量：80M\n类型：交叉注意力的编解码Transformer网络，依赖一个多尺度的全卷积视觉骨干进行视觉处理（在仿真器中预训练初始化*）\n输入：潜空间向量、RGB图片、关节角\n输出：高频机器人动作（关节角）\n输出频率：200Hz\n作用：快速灵巧的控制策略，结合图像编码和当前关节角信息，将潜空间向量表示转化成连续的机器人动作（目标关节角）\n训练细节： 训练数据：约500h的高质量、多机器人、多操作员的多样化遥操作行为数据集\n训练方法：基于raw pixel和文本指令到连续动作的映射，做端到端的训练，采用标准的回归损失。梯度从S1通过潜表示向量传递到S2，实现两者的协同优化。\n注意：\nS1和S2在训练阶段是耦合的 为了模拟真实的推理延迟，在训练阶段认为引入了S1和S2输入量的时间偏移* 调优的流式推理 S1和S2分别运行在一块专门的GPU上，基于共享的内存形成一个生产者-消费者模型。\nS2异步地处理最新的观测信息和自然语言指令， 持续地生成编码了高维意图行为意图的潜向量，并更新到共享内存中。\nS1在独立线程中，结合最新观测和自然语言指令，消费共享内存中的潜向量生成连续的机器人动作。由于S1比S2有更高的推理速度，因此有更高的时间分辨率，从而可以实现更紧密的闭环实时控制。\n数据流与传输带宽 数据流 Sensor以20Hz的频率采集图像和关节角。\nObservation（raw_img） ────传感器图像────>多尺度立体视觉网络 ────合并图像特征────> Observation（combind_img_token）\nObservation（command + state + combind_img_token）────79Hz观测信息────> S2 ────79Hz潜向量────> S1 ────200Hz动作────>执行器\nObservation（command + state + combind_img_token）─────20Hz观测信息────> S1\n"><meta property='og:url' content='https://RoboticsChen.github.io/articles/figure-ai-helix-system-paper-reading/'><meta property='og:site_name' content="RoboticsChen's Blog"><meta property='og:type' content='article'><meta property='article:section' content='Post'><meta property='article:tag' content='AI'><meta property='article:tag' content='Robotics'><meta property='article:published_time' content='2025-03-25T06:34:20+00:00'><meta property='article:modified_time' content='2025-03-25T06:34:20+00:00'><meta property='og:image' content='https://RoboticsChen.github.io/articles/figure-ai-helix-system-paper-reading/assets/image.png'><meta name=twitter:title content="Figure AI Helix System 论文总结"><meta name=twitter:description content="Figure AI Helix 系统总结 硬件组成 感知系统：Figure 02的头部、前躯干和后躯干共配备6个RGB摄像头 决策系统：嵌入式控制板及2块板载低功耗GPUs（NVIDIA RTX GPU，具体型号暂不明确）参考来源 执行系统：人形上身，腰部+头部+双臂共35个关节（从照片推测，头2 + 腰3 + 臂7x2 + 手16x2），部分手部自由度应该没有用到 网络结构 VLA网络由解耦的两个系统——预训练视觉语言模型（S2） + 控制小模型（S1）组成，两者在各自的时间尺度上运行，结合VLM模型广泛通用但不够快的特点和视觉运动策略快速而不广泛的特点，实现一个既广泛又快速的VLA控制模型。\nS2 参数量：7B\n类型：开源VLM\n输入：RGB图片、关节角信息\n输出：潜空间向量\n输出频率：7-9Hz\n作用：场景理解和语义理解，提供跨物体和场景的泛化能力，将所有与语义任务相关的信息提炼到一个单一的连续潜在向量中，并将其传递给S1\nS1 参数量：80M\n类型：交叉注意力的编解码Transformer网络，依赖一个多尺度的全卷积视觉骨干进行视觉处理（在仿真器中预训练初始化*）\n输入：潜空间向量、RGB图片、关节角\n输出：高频机器人动作（关节角）\n输出频率：200Hz\n作用：快速灵巧的控制策略，结合图像编码和当前关节角信息，将潜空间向量表示转化成连续的机器人动作（目标关节角）\n训练细节： 训练数据：约500h的高质量、多机器人、多操作员的多样化遥操作行为数据集\n训练方法：基于raw pixel和文本指令到连续动作的映射，做端到端的训练，采用标准的回归损失。梯度从S1通过潜表示向量传递到S2，实现两者的协同优化。\n注意：\nS1和S2在训练阶段是耦合的 为了模拟真实的推理延迟，在训练阶段认为引入了S1和S2输入量的时间偏移* 调优的流式推理 S1和S2分别运行在一块专门的GPU上，基于共享的内存形成一个生产者-消费者模型。\nS2异步地处理最新的观测信息和自然语言指令， 持续地生成编码了高维意图行为意图的潜向量，并更新到共享内存中。\nS1在独立线程中，结合最新观测和自然语言指令，消费共享内存中的潜向量生成连续的机器人动作。由于S1比S2有更高的推理速度，因此有更高的时间分辨率，从而可以实现更紧密的闭环实时控制。\n数据流与传输带宽 数据流 Sensor以20Hz的频率采集图像和关节角。\nObservation（raw_img） ────传感器图像────>多尺度立体视觉网络 ────合并图像特征────> Observation（combind_img_token）\nObservation（command + state + combind_img_token）────79Hz观测信息────> S2 ────79Hz潜向量────> S1 ────200Hz动作────>执行器\nObservation（command + state + combind_img_token）─────20Hz观测信息────> S1\n"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content='https://RoboticsChen.github.io/articles/figure-ai-helix-system-paper-reading/assets/image.png'><link rel="shortcut icon" href=/favicon.png></head><body class=article-page><script>(function(){const e="StackColorScheme";localStorage.getItem(e)||localStorage.setItem(e,"auto")})()</script><script>(function(){const t="StackColorScheme",e=localStorage.getItem(t),n=window.matchMedia("(prefers-color-scheme: dark)").matches===!0;e=="dark"||e==="auto"&&n?document.documentElement.dataset.scheme="dark":document.documentElement.dataset.scheme="light"})()</script><div class="container main-container flex on-phone--column extended"><aside class="sidebar left-sidebar sticky"><button class="hamburger hamburger--spin" type=button id=toggle-menu aria-label=切换菜单>
<span class=hamburger-box><span class=hamburger-inner></span></span></button><header><figure class=site-avatar><a href=/><img src=/img/avatar_hu_344c707452b06f8f.jpg width=300 height=300 class=site-logo loading=lazy alt=Avatar>
</a><span class=emoji>🦾</span></figure><div class=site-meta><h1 class=site-name><a href=/>RoboticsChen's Blog</a></h1><h2 class=site-description>一个机器人工程师的成长笔记</h2></div></header><ol class=menu-social><li><a href=https://github.com target=_blank title=GitHub rel=me><svg class="icon icon-tabler icon-tabler-brand-github" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M9 19c-4.3 1.4-4.3-2.5-6-3m12 5v-3.5c0-1 .1-1.4-.5-2 2.8-.3 5.5-1.4 5.5-6a4.6 4.6.0 00-1.3-3.2 4.2 4.2.0 00-.1-3.2s-1.1-.3-3.5 1.3a12.3 12.3.0 00-6.2.0C6.5 2.8 5.4 3.1 5.4 3.1a4.2 4.2.0 00-.1 3.2A4.6 4.6.0 004 9.5c0 4.6 2.7 5.7 5.5 6-.6.6-.6 1.2-.5 2V21"/></svg></a></li><li><a href=https://twitter.com target=_blank title=Twitter rel=me><svg class="icon icon-tabler icon-tabler-brand-twitter" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M22 4.01c-1 .49-1.98.689-3 .99-1.121-1.265-2.783-1.335-4.38-.737S11.977 6.323 12 8v1c-3.245.083-6.135-1.395-8-4 0 0-4.182 7.433 4 11-1.872 1.247-3.739 2.088-6 2 3.308 1.803 6.913 2.423 10.034 1.517 3.58-1.04 6.522-3.723 7.651-7.742a13.84 13.84.0 00.497-3.753C20.18 7.773 21.692 5.25 22 4.009z"/></svg></a></li><li><a href=https://www.emojisearch.app/ target=_blank title=serach-emojis rel=me><svg class="icon icon-tabler icon-tabler-search" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="10" cy="10" r="7"/><line x1="21" y1="21" x2="15" y2="15"/></svg></a></li><li><a href=https://cn.piliapp.com/symbol/ target=_blank title=serach-symbals rel=me><svg class="icon icon-tabler icon-tabler-hash" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><line x1="5" y1="9" x2="19" y2="9"/><line x1="5" y1="15" x2="19" y2="15"/><line x1="11" y1="4" x2="7" y2="20"/><line x1="17" y1="4" x2="13" y2="20"/></svg></a></li></ol><ol class=menu id=main-menu><li><a href=/><svg class="icon icon-tabler icon-tabler-home" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><polyline points="5 12 3 12 12 3 21 12 19 12"/><path d="M5 12v7a2 2 0 002 2h10a2 2 0 002-2v-7"/><path d="M9 21v-6a2 2 0 012-2h2a2 2 0 012 2v6"/></svg>
<span>Home</span></a></li><li><a href=/archives/><svg class="icon icon-tabler icon-tabler-archive" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><rect x="3" y="4" width="18" height="4" rx="2"/><path d="M5 8v10a2 2 0 002 2h10a2 2 0 002-2V8"/><line x1="10" y1="12" x2="14" y2="12"/></svg>
<span>Archives</span></a></li><li><a href=/search/><svg class="icon icon-tabler icon-tabler-search" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="10" cy="10" r="7"/><line x1="21" y1="21" x2="15" y2="15"/></svg>
<span>Search</span></a></li><li><a href=/links/><svg class="icon icon-tabler icon-tabler-link" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><path d="M10 14a3.5 3.5.0 005 0l4-4a3.5 3.5.0 00-5-5l-.5.5"/><path d="M14 10a3.5 3.5.0 00-5 0l-4 4a3.5 3.5.0 005 5l.5-.5"/></svg>
<span>Links</span></a></li><li class=menu-bottom-section><ol class=menu><li id=dark-mode-toggle><svg class="icon icon-tabler icon-tabler-toggle-left" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="8" cy="12" r="2"/><rect x="2" y="6" width="20" height="12" rx="6"/></svg>
<svg class="icon icon-tabler icon-tabler-toggle-right" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="16" cy="12" r="2"/><rect x="2" y="6" width="20" height="12" rx="6"/></svg>
<span>暗色模式</span></li></ol></li></ol></aside><aside class="sidebar right-sidebar sticky"><section class="widget archives"><div class=widget-icon><svg class="icon icon-tabler icon-tabler-hash" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><line x1="5" y1="9" x2="19" y2="9"/><line x1="5" y1="15" x2="19" y2="15"/><line x1="11" y1="4" x2="7" y2="20"/><line x1="17" y1="4" x2="13" y2="20"/></svg></div><h2 class="widget-title section-title">目录</h2><div class=widget--toc><nav id=TableOfContents><ol><li><a href=#硬件组成>硬件组成</a></li><li><a href=#网络结构>网络结构</a><ol><li><a href=#s2>S2</a></li><li><a href=#s1>S1</a></li><li><a href=#训练细节>训练细节：</a></li><li><a href=#调优的流式推理>调优的流式推理</a></li></ol></li><li><a href=#数据流与传输带宽>数据流与传输带宽</a><ol><li><a href=#数据流>数据流</a></li><li><a href=#传输带宽>传输带宽</a></li></ol></li><li><a href=#特色>特色</a></li></ol></nav></div></section></aside><main class="main full-width"><article class="has-image main-article"><header class=article-header><div class=article-image><a href=/articles/figure-ai-helix-system-paper-reading/><img src=/articles/figure-ai-helix-system-paper-reading/assets/image_hu_634c4531830c33b2.png srcset="/articles/figure-ai-helix-system-paper-reading/assets/image_hu_634c4531830c33b2.png 800w, /articles/figure-ai-helix-system-paper-reading/assets/image_hu_34b5a2a216b48b39.png 1600w" width=800 height=297 loading=lazy alt="Featured image of post Figure AI Helix System 论文总结"></a></div><div class=article-details><header class=article-category><a href=/categories/paper-reading/>Paper-Reading</a></header><div class=article-title-wrapper><h2 class=article-title><a href=/articles/figure-ai-helix-system-paper-reading/>Figure AI Helix System 论文总结</a></h2></div><footer class=article-time><div><svg class="icon icon-tabler icon-tabler-calendar-time" width="56" height="56" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><path d="M11.795 21H5a2 2 0 01-2-2V7a2 2 0 012-2h12a2 2 0 012 2v4"/><circle cx="18" cy="18" r="4"/><path d="M15 3v4"/><path d="M7 3v4"/><path d="M3 11h16"/><path d="M18 16.496V18l1 1"/></svg>
<time class=article-time--published>Mar 25, 2025</time></div><div><svg class="icon icon-tabler icon-tabler-clock" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="12" r="9"/><polyline points="12 7 12 12 15 15"/></svg>
<time class=article-time--reading>阅读时长: 1 分钟</time></div></footer></div></header><section class=article-content><h1 id=figure-ai-helix-系统总结>Figure AI Helix 系统总结</h1><h2 id=硬件组成>硬件组成</h2><p><img src=/articles/figure-ai-helix-system-paper-reading/assets/image-2.png width=753 height=822 srcset="/articles/figure-ai-helix-system-paper-reading/assets/image-2_hu_a4795c140fc72dc6.png 480w, /articles/figure-ai-helix-system-paper-reading/assets/image-2_hu_d64487326dfbd911.png 1024w" loading=lazy alt=头部摄像头 class=gallery-image data-flex-grow=91 data-flex-basis=219px> <img src=/articles/figure-ai-helix-system-paper-reading/assets/image-1.png width=514 height=425 srcset="/articles/figure-ai-helix-system-paper-reading/assets/image-1_hu_4534d8beb6940b26.png 480w, /articles/figure-ai-helix-system-paper-reading/assets/image-1_hu_ed1385affa40c429.png 1024w" loading=lazy alt=机器人整体照片 class=gallery-image data-flex-grow=120 data-flex-basis=290px></p><ol><li>感知系统：Figure 02的头部、前躯干和后躯干共配备6个RGB摄像头</li><li>决策系统：嵌入式控制板及2块板载低功耗GPUs（NVIDIA RTX GPU，具体型号暂不明确）<a class=link href=https://blogs.nvidia.com/blog/figure-humanoid-robot-autonomous/ target=_blank rel=noopener>参考来源</a></li><li>执行系统：人形上身，腰部+头部+双臂共35个关节（从照片推测，头2 + 腰3 + 臂7x2 + 手16x2），部分手部自由度应该没有用到</li></ol><h2 id=网络结构>网络结构</h2><p><img src=/articles/figure-ai-helix-system-paper-reading/assets/image.png width=1448 height=537 srcset="/articles/figure-ai-helix-system-paper-reading/assets/image_hu_3114dd015cc042f.png 480w, /articles/figure-ai-helix-system-paper-reading/assets/image_hu_f773820ef4c3694b.png 1024w" loading=lazy alt=Pipeline class=gallery-image data-flex-grow=269 data-flex-basis=647px></p><p>VLA网络由解耦的两个系统——预训练视觉语言模型（S2） + 控制小模型（S1）组成，两者在各自的时间尺度上运行，结合VLM模型广泛通用但不够快的特点和视觉运动策略快速而不广泛的特点，实现一个既广泛又快速的VLA控制模型。</p><h3 id=s2>S2</h3><p>参数量：7B<br>类型：开源VLM<br>输入：RGB图片、关节角信息<br>输出：潜空间向量<br>输出频率：7-9Hz<br>作用：场景理解和语义理解，提供跨物体和场景的泛化能力，将所有与语义任务相关的信息提炼到一个单一的连续潜在向量中，并将其传递给S1</p><h3 id=s1>S1</h3><p>参数量：80M<br>类型：交叉注意力的编解码Transformer网络，依赖一个多尺度的全卷积视觉骨干进行视觉处理（在仿真器中预训练初始化*）<br>输入：潜空间向量、RGB图片、关节角<br>输出：高频机器人动作（关节角）<br>输出频率：200Hz<br>作用：快速灵巧的控制策略，结合图像编码和当前关节角信息，将潜空间向量表示转化成连续的机器人动作（目标关节角）</p><h3 id=训练细节>训练细节：</h3><p>训练数据：约500h的高质量、多机器人、多操作员的多样化遥操作行为数据集<br>训练方法：基于raw pixel和文本指令到连续动作的映射，做端到端的训练，采用标准的回归损失。梯度从S1通过潜表示向量传递到S2，实现两者的协同优化。</p><blockquote><p>注意：</p><ul><li>S1和S2在训练阶段是耦合的</li><li>为了模拟真实的推理延迟，在训练阶段认为引入了S1和S2输入量的时间偏移*</li></ul></blockquote><h3 id=调优的流式推理>调优的流式推理</h3><p>S1和S2分别运行在一块专门的GPU上，基于共享的内存形成一个生产者-消费者模型。<br>S2异步地处理最新的观测信息和自然语言指令， 持续地生成编码了高维意图行为意图的潜向量，并更新到共享内存中。<br>S1在独立线程中，结合最新观测和自然语言指令，消费共享内存中的潜向量生成连续的机器人动作。由于S1比S2有更高的推理速度，因此有更高的时间分辨率，从而可以实现更紧密的闭环实时控制。</p><h2 id=数据流与传输带宽>数据流与传输带宽</h2><h3 id=数据流>数据流</h3><p><img src=/articles/figure-ai-helix-system-paper-reading/assets/image-4.png width=1080 height=315 srcset="/articles/figure-ai-helix-system-paper-reading/assets/image-4_hu_b4c9380bafbeb227.png 480w, /articles/figure-ai-helix-system-paper-reading/assets/image-4_hu_3e4bb4a4fa05811.png 1024w" loading=lazy alt=图像特征融合 class=gallery-image data-flex-grow=342 data-flex-basis=822px><br>Sensor以20Hz的频率采集图像和关节角。<br>Observation（raw_img） ────传感器图像────>多尺度立体视觉网络 ────合并图像特征────> Observation（combind_img_token）<br>Observation（command + state + combind_img_token）────7<del>9Hz观测信息────> S2 ────7</del>9Hz潜向量────> S1 ────200Hz动作────>执行器<br>Observation（command + state + combind_img_token）─────20Hz观测信息────> S1</p><h3 id=传输带宽>传输带宽</h3><p>未提及</p><h2 id=特色>特色</h2><p><a class=link href=https://www.figure.ai/news/helix target=_blank rel=noopener>官方观点</a>：</p><ol><li>全身控制：它是历史上第一个类人机器人上半身的高速连续控制 VLA 模型，覆盖手腕、躯干、头部和单个手指；</li><li>多机器人协作：可以两台机器人用同样的模型控制协作，完成前所未见的任务；</li><li>抓取任何物品：可以捡起任何小型物体，包括数千种它们从未遇到过的物品，只需遵循自然语言指令即可；</li><li>单一神经网络：Helix 使用一组神经网络权重来学习所有行为，如抓取和放置物品、使用抽屉和冰箱、以及跨机器人交互，而无需任何任务特定的微调；</li><li>本地化：Helix 是史上第一个在板端 GPU 运行的机器人 VLA 模型，已经具备了商业化落地能力。</li><li>扩展到家用场景成本低：相比于传统的依据单个任务由机器人专家定制化的控制和需要采集大量数据的模仿学习控制，借助VLM模型实现强大泛化能力的端到端模型更有优势
<img src=/articles/figure-ai-helix-system-paper-reading/assets/image-3.png width=1920 height=1081 srcset="/articles/figure-ai-helix-system-paper-reading/assets/image-3_hu_67e434179e12c82a.png 480w, /articles/figure-ai-helix-system-paper-reading/assets/image-3_hu_51eb2eb58bdefd4f.png 1024w" loading=lazy alt="机器人Scaling Law" class=gallery-image data-flex-grow=177 data-flex-basis=426px></li></ol><p><a class=link href=https://mp.weixin.qq.com/s/ZkY0IYETGXXDixuo-wKekA target=_blank rel=noopener>量子位观点</a></p><ol><li>空间感知：多相机实现隐式立体视觉与多尺度视觉表示，增强了3D空间感知和场景理解精度</li><li>执行速度上限高：在物流场景的微调应用中使用简单的test-time加速技术（同样的waypoint以更短的时间间隔执行），保持高成功率的同时实现了更快的执行速度。</li><li>微调成本低：仅用8小时精心挑选的数据就能训练出一个灵活且适应性强的策略。</li><li>引入视觉自校准模型：该模型可以让每个机器人通过自身的视觉输入来自我校准，估算出机械臂末端的精确位置和姿态，提高跨机器人实例的泛化能力。</li><li>存在自纠正能力：训练过程中，Figure排除了那些较慢的、遗漏的或失败的案例，不过特意保留了包含纠正行为的案例。</li><li>默认交互方式为语音交互
<img src=/articles/figure-ai-helix-system-paper-reading/assets/image-5.gif width=640 height=328 srcset="/articles/figure-ai-helix-system-paper-reading/assets/image-5_hu_9100231fc2360407.gif 480w, /articles/figure-ai-helix-system-paper-reading/assets/image-5_hu_ec57953f01cb7d02.gif 1024w" loading=lazy alt=自纠正 class=gallery-image data-flex-grow=195 data-flex-basis=468px></li></ol><p>个人判断：</p><ol><li>采用七自由度冗余机械臂，工作空间更大，由于是端到端的模型，不存在FK/IK，多一个自由度对模型来说区别不大，但效果会好很多</li><li>暂不能实现跨构型的泛化能力，但是针对不同的执行器和机器人构型，不需要改变模型架构，只需要改变输出参数的数量重新采集数据训练模型</li></ol></section><footer class=article-footer><section class=article-tags><a href=/tags/ai/>AI</a>
<a href=/tags/robotics/>Robotics</a></section><section class=article-copyright><svg class="icon icon-tabler icon-tabler-copyright" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentcolor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="12" r="9"/><path d="M14.5 9a3.5 4 0 100 6"/></svg>
<span>Licensed under CC BY-NC-SA 4.0</span></section></footer><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css integrity=sha384-n8MVd4RsNIU0tAv4ct0nTaAbDJwPJzDEaqSD1odI+WdtXRGWt2kTvGFasHpSy3SV crossorigin=anonymous><script src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js integrity=sha384-XjKyOOlGwcjNTAIQHIpgOno0Hl1YQqzUOEleOLALmuqehneUG+vnGctmUb0ZY0l8 crossorigin=anonymous defer></script><script src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 crossorigin=anonymous defer></script><script>window.addEventListener("DOMContentLoaded",()=>{const e=document.querySelector(".main-article");renderMathInElement(e,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}],ignoredClasses:["gist"]})})</script></article><footer class=site-footer><section class=copyright>&copy;
2024 -
2025 RoboticsChen's Blog</section><section class=powerby>一个机器人工程师的成长笔记<br>使用 <a href=https://gohugo.io/ target=_blank rel=noopener>Hugo</a> 构建<br>主题 <b><a href=https://github.com/CaiJimmy/hugo-theme-stack target=_blank rel=noopener data-version=3.30.0>Stack</a></b> 由 <a href=https://jimmycai.com target=_blank rel=noopener>Jimmy</a> 设计</section></footer><div class=pswp tabindex=-1 role=dialog aria-hidden=true><div class=pswp__bg></div><div class=pswp__scroll-wrap><div class=pswp__container><div class=pswp__item></div><div class=pswp__item></div><div class=pswp__item></div></div><div class="pswp__ui pswp__ui--hidden"><div class=pswp__top-bar><div class=pswp__counter></div><button class="pswp__button pswp__button--close" title="Close (Esc)"></button>
<button class="pswp__button pswp__button--share" title=Share></button>
<button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>
<button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button><div class=pswp__preloader><div class=pswp__preloader__icn><div class=pswp__preloader__cut><div class=pswp__preloader__donut></div></div></div></div></div><div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap"><div class=pswp__share-tooltip></div></div><button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
</button>
<button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)"></button><div class=pswp__caption><div class=pswp__caption__center></div></div></div></div></div><script src=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.js integrity="sha256-ePwmChbbvXbsO02lbM3HoHbSHTHFAeChekF1xKJdleo=" crossorigin=anonymous defer></script><script src=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe-ui-default.min.js integrity="sha256-UKkzOn/w1mBxRmLLGrSeyB4e1xbrp4xylgAWb3M42pU=" crossorigin=anonymous defer></script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/default-skin/default-skin.min.css crossorigin=anonymous><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.css crossorigin=anonymous></main></div><script src=https://cdn.jsdelivr.net/npm/node-vibrant@3.1.6/dist/vibrant.min.js integrity="sha256-awcR2jno4kI5X0zL8ex0vi2z+KMkF24hUW8WePSA9HM=" crossorigin=anonymous></script><script type=text/javascript src=/ts/main.1e9a3bafd846ced4c345d084b355fb8c7bae75701c338f8a1f8a82c780137826.js defer></script><script type=text/javascript src=/ts/custom.ab98a087394b20c3627bde9d8672bce1b06ea80e7eb4746e409fccaa1c39dd7b.js defer></script><script>(function(){const e=document.createElement("link");e.href="https://fonts.googleapis.com/css2?family=Lato:wght@300;400;700&display=swap",e.type="text/css",e.rel="stylesheet",document.head.appendChild(e)})()</script></body></html>