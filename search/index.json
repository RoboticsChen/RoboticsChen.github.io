[{"content":"Figure AI Helix 系统总结 硬件组成 感知系统：Figure 02的头部、前躯干和后躯干共配备6个RGB摄像头 决策系统：嵌入式控制板及2块板载低功耗GPUs（NVIDIA RTX GPU，具体型号暂不明确）参考来源 执行系统：人形上身，腰部+头部+双臂共35个关节（从照片推测，头2 + 腰3 + 臂7x2 + 手16x2），部分手部自由度应该没有用到 网络结构 VLA网络由解耦的两个系统——预训练视觉语言模型（S2） + 控制小模型（S1）组成，两者在各自的时间尺度上运行，结合VLM模型广泛通用但不够快的特点和视觉运动策略快速而不广泛的特点，实现一个既广泛又快速的VLA控制模型。\nS2 参数量：7B\n类型：开源VLM\n输入：RGB图片、关节角信息\n输出：潜空间向量\n输出频率：7-9Hz\n作用：场景理解和语义理解，提供跨物体和场景的泛化能力，将所有与语义任务相关的信息提炼到一个单一的连续潜在向量中，并将其传递给S1\nS1 参数量：80M\n类型：交叉注意力的编解码Transformer网络，依赖一个多尺度的全卷积视觉骨干进行视觉处理（在仿真器中预训练初始化*）\n输入：潜空间向量、RGB图片、关节角\n输出：高频机器人动作（关节角）\n输出频率：200Hz\n作用：快速灵巧的控制策略，结合图像编码和当前关节角信息，将潜空间向量表示转化成连续的机器人动作（目标关节角）\n训练细节： 训练数据：约500h的高质量、多机器人、多操作员的多样化遥操作行为数据集\n训练方法：基于raw pixel和文本指令到连续动作的映射，做端到端的训练，采用标准的回归损失。梯度从S1通过潜表示向量传递到S2，实现两者的协同优化。\n注意：\nS1和S2在训练阶段是耦合的 为了模拟真实的推理延迟，在训练阶段认为引入了S1和S2输入量的时间偏移* 调优的流式推理 S1和S2分别运行在一块专门的GPU上，基于共享的内存形成一个生产者-消费者模型。\nS2异步地处理最新的观测信息和自然语言指令， 持续地生成编码了高维意图行为意图的潜向量，并更新到共享内存中。\nS1在独立线程中，结合最新观测和自然语言指令，消费共享内存中的潜向量生成连续的机器人动作。由于S1比S2有更高的推理速度，因此有更高的时间分辨率，从而可以实现更紧密的闭环实时控制。\n数据流与传输带宽 数据流 Sensor以20Hz的频率采集图像和关节角。\nObservation（raw_img） ────传感器图像────\u0026gt;多尺度立体视觉网络 ────合并图像特征────\u0026gt; Observation（combind_img_token）\nObservation（command + state + combind_img_token）────79Hz观测信息────\u0026gt; S2 ────79Hz潜向量────\u0026gt; S1 ────200Hz动作────\u0026gt;执行器\nObservation（command + state + combind_img_token）─────20Hz观测信息────\u0026gt; S1\n传输带宽 未提及\n特色 官方观点：\n全身控制：它是历史上第一个类人机器人上半身的高速连续控制 VLA 模型，覆盖手腕、躯干、头部和单个手指； 多机器人协作：可以两台机器人用同样的模型控制协作，完成前所未见的任务； 抓取任何物品：可以捡起任何小型物体，包括数千种它们从未遇到过的物品，只需遵循自然语言指令即可； 单一神经网络：Helix 使用一组神经网络权重来学习所有行为，如抓取和放置物品、使用抽屉和冰箱、以及跨机器人交互，而无需任何任务特定的微调； 本地化：Helix 是史上第一个在板端 GPU 运行的机器人 VLA 模型，已经具备了商业化落地能力。 扩展到家用场景成本低：相比于传统的依据单个任务由机器人专家定制化的控制和需要采集大量数据的模仿学习控制，借助VLM模型实现强大泛化能力的端到端模型更有优势 量子位观点\n空间感知：多相机实现隐式立体视觉与多尺度视觉表示，增强了3D空间感知和场景理解精度 执行速度上限高：在物流场景的微调应用中使用简单的test-time加速技术（同样的waypoint以更短的时间间隔执行），保持高成功率的同时实现了更快的执行速度。 微调成本低：仅用8小时精心挑选的数据就能训练出一个灵活且适应性强的策略。 引入视觉自校准模型：该模型可以让每个机器人通过自身的视觉输入来自我校准，估算出机械臂末端的精确位置和姿态，提高跨机器人实例的泛化能力。 存在自纠正能力：训练过程中，Figure排除了那些较慢的、遗漏的或失败的案例，不过特意保留了包含纠正行为的案例。 默认交互方式为语音交互 个人判断：\n采用七自由度冗余机械臂，工作空间更大，由于是端到端的模型，不存在FK/IK，多一个自由度对模型来说区别不大，但效果会好很多 暂不能实现跨构型的泛化能力，但是针对不同的执行器和机器人构型，不需要改变模型架构，只需要改变输出参数的数量重新采集数据训练模型 ","date":"2025-03-25T06:34:20.467Z","image":"https://RoboticsChen.github.io/articles/figure-ai-helix-system-paper-reading/assets/image_hu_745518e71581943d.png","permalink":"https://RoboticsChen.github.io/articles/figure-ai-helix-system-paper-reading/","title":"Figure AI Helix System 论文总结"},{"content":"安装依赖 perf：通常已经安装在大多数 Linux 系统中。\n1 sudo apt-get install linux-tools-common linux-tools-generic Flamegraph：由 Brendan Gregg 创建的工具集，用于生成火焰图。\n1 2 3 4 5 git clone https://github.com/brendangregg/Flamegraph.git # 在合适的位置保存脚本 cd Flamegraph sudo chmod +x *.pl echo \u0026#39;export PATH=\u0026#34;$PATH\u0026#34;:\u0026#39;$(pwd) \u0026gt;\u0026gt; ~/.bashrc # 添加环境变量 source ~/.bashrc perf使用教程 使用 perf 工具进行性能采样， Ctrl+C结束。 1 2 3 4 sudo sysctl -w kernel.kptr_restrict=0 # 暂时允许对内核符号的访问 sudo sysctl -w kernel.perf_event_paranoid=-1 perf record -F 99 -a -g -- ./my_program F 99 设置采样频率为每秒 99 次。 a 表示对系统中的所有 CPU 进行采样。 g 表示启用调用图（call graph）收集。\n将这些数据转换为火焰图格式,再转化为折叠格式： 1 perf script \u0026gt; out.perf 转到 Flamegraph 的目录，将 out.perf 中的性能数据转换为折叠格式，并输出到 out.folded 文件。 1 ./stackcollapse-perf.pl out.perf \u0026gt; out.folded 生成火焰图： 1 ./flamegraph.pl out.folded \u0026gt; flamegraph.svg 将火焰图拖拽到浏览器即可查看 一键perf分析脚本 展开复制保存为`perf_it.sh` ✖ 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 #!/bin/bash # 确保传入的命令参数 if [ -z \u0026#34;$1\u0026#34; ]; then echo \u0026#34;Usage: $0 \u0026lt;command_to_profile\u0026gt;\u0026#34; exit 1 fi # 暂时允许对内核符号的访问 sudo sysctl -w kernel.kptr_restrict=0 sudo sysctl -w kernel.perf_event_paranoid=-1 # 设置信号处理：捕获 Ctrl+C 后继续执行 cleanup() { if [ -f \u0026#34;perf.data\u0026#34; ]; then echo -e \u0026#34;\\n捕获到中断信号，正在生成火焰图...\u0026#34; generate_flamegraph else echo -e \u0026#34;\\n捕获到中断信号，但未生成 perf 数据。\u0026#34; fi exit 0 } generate_flamegraph() { # 生成时间戳目录 export tmp_time=$(date +\u0026#34;%Y%m%d_%H%M%S\u0026#34;) export tmp_dir=\u0026#34;flame_$tmp_time\u0026#34; mkdir -p \u0026#34;$tmp_dir\u0026#34; # 移动 perf.data 到目标目录 mv perf.data \u0026#34;$tmp_dir/\u0026#34; || { echo \u0026#34;错误: 未找到 perf.data 文件\u0026#34; exit 1 } cd \u0026#34;$tmp_dir\u0026#34; || exit 1 # 生成 perf 数据 echo \u0026#34;正在处理 perf 数据...\u0026#34; perf script \u0026gt; \u0026#34;$tmp_time.perf\u0026#34; || { echo \u0026#34;错误: perf script 执行失败\u0026#34; exit 1 } # 生成折叠数据 stackcollapse-perf.pl \u0026#34;$tmp_time.perf\u0026#34; \u0026gt; \u0026#34;$tmp_time.folded\u0026#34; || { echo \u0026#34;错误: stackcollapse-perf.pl 执行失败\u0026#34; exit 1 } # 生成火焰图 flamegraph.pl \u0026#34;$tmp_time.folded\u0026#34; \u0026gt; \u0026#34;flamegraph_$tmp_time.svg\u0026#34; echo \u0026#34;🔥 火焰图生成完毕：$PWD/flamegraph_$tmp_time.svg\u0026#34; } # 注册信号处理 trap cleanup SIGINT SIGTERM # 运行 perf record 并检查退出状态 echo \u0026#34;开始性能采集 (按 Ctrl+C 停止)...\u0026#34; if perf record -F 99 -a -g -- \u0026#34;$@\u0026#34;; then # 仅当 perf record 成功时生成火焰图 generate_flamegraph else echo \u0026#34;错误: 目标命令执行失败，跳过火焰图生成\u0026#34; rm -f perf.data # 清理可能的无效数据 exit 1 fi 用法：\n1 ./perf_it ./my_program \u0026lt;args...\u0026gt; ","date":"2025-03-14T01:40:52.569Z","image":"https://RoboticsChen.github.io/articles/perf%E6%80%A7%E8%83%BD%E5%88%86%E6%9E%90%E5%B7%A5%E5%85%B7%E7%9A%84%E4%BD%BF%E7%94%A8/image_hu_b8d08c7084d41963.png","permalink":"https://RoboticsChen.github.io/articles/perf%E6%80%A7%E8%83%BD%E5%88%86%E6%9E%90%E5%B7%A5%E5%85%B7%E7%9A%84%E4%BD%BF%E7%94%A8/","title":"perf性能分析工具的使用"},{"content":"0. 原理介绍 该方案先通过xclip获取用户选中的文本内容，对文本进行清理（移除隐藏字符,html标签,换行符），然后通过调用翻译 API（如谷歌翻译或其他服务）将文本翻译为目标语言，最后使用系统通知工具（notify-send）将翻译结果以通知的形式显示给用户。\n1. 安装依赖 1 sudo apt install xcilp jq xcilp 用于获取剪贴板内容 jq 用于对字符串进行 URL 编码（百分号编码）\n2. 编写脚本 在/opt目录下新建一个名为my_translate.sh的脚本文件\n1 sudo nano /opt/my_translate.sh 添加以下内容并保存退出：\n点击展开 ✖ 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 #!/bin/bash # 从剪贴板获取选中的文字 selected_text=$(xclip -o) clean_text=$(echo \u0026#34;$selected_text\u0026#34; | sed \u0026#39;s/\u0026lt;[^\u0026gt;]*\u0026gt;//g\u0026#39; | tr -d \u0026#39;\\n\\r\u0026#39; | tr -d \u0026#39;\\t\u0026#39; | tr -s \u0026#39; \u0026#39;) echo $clean_text # 检查目标语言参数 target_language=\u0026#34;$1\u0026#34; # 谷歌翻译API的URL API_URL=\u0026#34;https://translate.googleapis.com/translate_a/single?client=gtx\u0026amp;sl=auto\u0026amp;tl=$target_language\u0026amp;dt=t\u0026amp;q=\u0026#34; query=$(echo -n \u0026#34;$clean_text\u0026#34; | jq -sRr @uri) # 调用Google Translate API response=$(curl -sL \u0026#34;${API_URL}${query}\u0026#34;) ## just for debug #echo \u0026#34;完整请求 URL: ${API_URL}${query}\u0026#34; \u0026gt;\u0026gt; debug_log.txt #echo \u0026#34;API 响应: $response\u0026#34; \u0026gt;\u0026gt; debug_log.txt # 提取翻译结果 translation=$(echo $response | awk -F\u0026#39;\u0026#34;\u0026#39; \u0026#39;{print $2}\u0026#39;) echo $translation # 发送通知 notify-send \u0026#34;Translate Rerult ($target_language): \u0026#34; \u0026#34;$translation\u0026#34; 添加运行权限\n1 sudo chmod +x /opt/my_translate.sh 3.快捷键设置 在设置/键盘/键盘快捷键中添加两个自定义快捷键(en-\u0026gt;zh, zh-\u0026gt;en)：\n注意根据脚本名称对应修改命令\n","date":"2024-12-01T05:17:56.756Z","image":"https://RoboticsChen.github.io/articles/linux-ubuntu-wayland-google-translate/translate_hu_39d4c8b4dc6de913.png","permalink":"https://RoboticsChen.github.io/articles/linux-ubuntu-wayland-google-translate/","title":"Linux 划词谷歌翻译解决方案"},{"content":"相机成像与ISP 相机成像 CCD vs CMOS CCD：全称Charge Coupled Device，即电荷耦合器件 CMOS：全称Complementary Metal Oxide Semiconductor，即互补金属氧化物半导体 这两种传感器都由多个光电二极管组成的阵列，每个光电二极管的作用是将接收到的光信号转化为电荷。两者的基本工作原理相似，传感器接收光信号并通过一定时间的电荷积累实现曝光，进而获得每个像素的光强度数据。最后，这些电荷经模数转换被转换为电压并经过放大处理，最终生成原始的RAW图像数据。\n在具体的实现方式上，两者有一定的区别：\nCCD：每个像素点的电荷会被传输到一个单独的输出节点，通过这一节点将电荷转化为电压信号，即一行行读出。这种方式有助于实现较低的噪声和较高的图像质量，但由于传输过程中需要大量的电子元件和复杂的电路设计，CCD传感器的功耗较高，且读出速度较慢。 CMOS：每个像素点都具有一个独立的放大器，能够在传感器内直接将电荷转化为电压信号并进行放大处理。这种方式使得每个像素的电荷转换过程相对独立，能更快速地读取图像数据，降低功耗，但在低光照环境下可能会产生较多噪声。 总的来看，CMOS传感器相比于CCD传感器，工艺简单、成本更低，随着CMOS技术的不断进步，尤其是在降低噪声、提高低光照性能和增强图像质量方面的提升，CMOS传感器已经逐渐取代了传统的CCD传感器，成为主流选择。当今CCD传感器的应用主要局限于一些对图像质量要求极高的专业领域。\nBayer阵列与RAW图像 1.1节中提到，sensor通过将光强信号转化为电压信号，从而得到了RAW图像，因此，如果不加任何处理，得到的raw图将是只有亮度的黑白图像。\n为了合成彩色图像，则需要得到每一个像素点处的RGB三通道的亮度值。要获得某个通道的亮度值，可以在光电二极管前加上只允许红/绿/蓝色光通过的滤光片。于是我们可以采用这样一种排列方式，四个滤光片依次按R、G、G、B排列。为什么是两个绿色呢，因为人眼对绿色更为敏感，所以这里使用两个绿色来增强绿色的解析力。这种排列方式又称Bayer阵列（Bayer Pattern），于1974由柯达的工程师Bayer提出。\n除了最常用的Bayer阵列，一些厂商也采用了其他的排列方式，如索尼的REGB(红青绿蓝)，华为的RYYB(红黄黄蓝)，富士的乱七八糟排列（bushi\n如果此时，我们将这些bayer排列的电信号转换成数字信号，并保存下来，就可以得到RAW图像。\n此时的图像依旧是黑白的，并且如果放大局部，会发现图像会呈现马赛克一样的效果，为了得到最终的图像，需要完成一系列图像信号处理，即Image Signal Processing(ISP)。\nImage Signal Process（ISP） 坏点校正（Dead Pixel Correction, DPC） 根据是否会发生变化，坏点可以分为静态坏点和动态坏点。\n静态坏点：不随着时间、增益等改变，通常是sensor制造时因工艺原因产生的坏点。 动态坏点：因为增益、温度等引起的坏点，会随着时间变化而改变。 在 ISP 处理阶段，需要对图像中的坏点进行校正。静态坏点通常在传感器生产线上标定，其位置会被记录到 OTP（One Time Programmable）存储器中，用于后续校正。\n动态坏点的校正则发生在静态坏点校正完成之后。校正过程中，通过逐一检查每个像素点与其周围一定范围内的像素值，如果发现某个像素的值与周围像素的值差异超过设定阈值，则判定为坏点。对于这些坏点，通常使用同通道内周围像素的平均值来替代进行校正，从而恢复图像的完整性和质量。\n示例代码 ✖ 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 /** * @brief Performs dead pixel correction (DPC) on RAW image data by identifying * and correcting pixels that differ significantly from their neighbors. * * @param raw Reference to the input RAW image data. * @param thres Threshold value for detecting dead pixels. A pixel is * considered dead if it deviates from its neighboring pixels by more than this * threshold. * @param mode DPC correction mode. Options are: * - DPC_MODE_MEAN: Replaces the dead pixel value with the * average of its immediate neighbors. * - DPC_MODE_GRADIENT: Replaces the dead pixel based on the * smallest gradient among neighboring pixels. * - DPC_MODE_UNKNOWN: Throws an error if the mode is * unrecognized. * @param clip Maximum allowable pixel value after correction. Values * exceeding this are clipped. **/ void DPC(ImageRaw\u0026amp; raw, uint16_t thres, DPC_MODE mode, uint16_t clip) { ImageRaw* raw_pad = new ImageRaw(raw); raw_pad-\u0026gt;padding(2, PADDING_MODE_REFLECT); for (int y = 0; y \u0026lt; raw_pad-\u0026gt;getHeight() - 4; y++) { for (int x = 0; x \u0026lt; raw_pad-\u0026gt;getWidth() - 4; x++) { uint16_t p0, p1, p2, p3, p4, p5, p6, p7, p8; uint16_t dv, dh, ddl, ddr, minimal; p0 = raw_pad-\u0026gt;at(y + 2, x + 2); p1 = raw_pad-\u0026gt;at(y, x); p2 = raw_pad-\u0026gt;at(y, x + 2); p3 = raw_pad-\u0026gt;at(y, x + 4); p4 = raw_pad-\u0026gt;at(y + 2, x); p5 = raw_pad-\u0026gt;at(y + 2, x + 4); p6 = raw_pad-\u0026gt;at(y + 4, x); p7 = raw_pad-\u0026gt;at(y + 4, x + 2); p8 = raw_pad-\u0026gt;at(y + 4, x + 4); if ((ABS(p1 - p0) \u0026gt; thres) \u0026amp;\u0026amp; (ABS(p2 - p0) \u0026gt; thres) \u0026amp;\u0026amp; (ABS(p3 - p0) \u0026gt; thres) \u0026amp;\u0026amp; (ABS(p4 - p0) \u0026gt; thres) \u0026amp;\u0026amp; (ABS(p5 - p0) \u0026gt; thres) \u0026amp;\u0026amp; (ABS(p6 - p0) \u0026gt; thres) \u0026amp;\u0026amp; (ABS(p7 - p0) \u0026gt; thres) \u0026amp;\u0026amp; (ABS(p8 - p0) \u0026gt; thres)) { switch (mode) { case DPC_MODE_MEAN: p0 = (p2 + p4 + p5 + p7) / 4; break; case DPC_MODE_GRADIENT: dv\t= ABS(2 * p0 - p2 - p7); dh\t= ABS(2 * p0 - p4 - p5); ddl\t= ABS(2 * p0 - p1 - p8); ddr\t= ABS(2 * p0 - p3 - p6); minimal\t= MIN(MIN(MIN(dv, dh), ddl), ddr); if (minimal == dv)\tp0 = (p2 + p7 + 1) / 2; // Adding +1 here helps prevent rounding errors in integer division. else if (minimal == dh)\tp0 = (p4 + p5 + 1) / 2; else if (minimal == ddl)\tp0 = (p1 + p8 + 1) / 2; else\tp0 = (p3 + p6 + 1) / 2; break; case DPC_MODE_UNKNOWN: TRACE_DEBUG_LOG_ERROR(\u0026#34;Unknown DPC Mode:%s\\n\u0026#34;, mode); throw \u0026#34;Unknown DPC mode\u0026#34;; break; default: TRACE_DEBUG_LOG_ERROR(\u0026#34;Unknown DPC Mode:%s\\n\u0026#34;, mode); throw \u0026#34;Unknown DPC mode\u0026#34;; break; } raw.at(y, x) = p0; } } } raw.clip(0, clip); delete raw_pad; } 暗电流校正（Black level Correction，BLC） 暗电流校正又称“黑度校正”。在相机工作过程中，即使没有接收到光信号，传感器也会由于某些因素产生一定的暗电流，从而导致输出的电压信号存在偏移，图像的“黑度”与真实情况存在差异。这个偏移值的来源有两方面，其一是模数转换千的增益，其二是温度等物理原因导致的传感器偏移。黑电平校正模块就是通过标定的方式，确定这个偏移量的具体值。后续的 ISP 处理模块，需要先减掉该偏移值，才能保证图像的准确性。\n模拟信号很微弱时，有可能不被 A/D 转换出来，导致光线很暗时，图像细节丢失。因此，Sesnor 会在 A/D 转换前，给模拟信号一个固定的偏移量，保证输出的数字信号保留更多的图像细节。 通常的校正方法是在传感器完全遮光的情况下捕获“暗帧”。然后记录每个像素的数字输出，计算该图像的平均值（有时使用中值）。这个平均值或中值表示传感器的整体黑电平偏移。接下来，将该值从传感器生成的图像中的每个像素值中减去，以校正暗电流引起的偏移，恢复图像的真实亮度。\n示例代码 ✖ 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 /** * @brief Performs black level correction (BLC) on RAW image data based on the * specified Bayer pattern. * * @param raw Reference to the input RAW image data. * @param r Black level correction value for the red channel. * @param gr Black level correction value for the green channel (red * row). * @param gb Black level correction value for the green channel (blue * row). * @param b Black level correction value for the blue channel. * @param alpha Alpha coefficient for adjusting the green channel (red * row). * @param beta Beta coefficient for adjusting the green channel (blue * row). * @param bayer_pattern The Bayer pattern used in the RAW data (e.g., RGGB, * BGGR, GBRG, GRBG). * @param clip Maximum allowable pixel value after correction. Values * exceeding this are clipped. */ void BLC(ImageRaw\u0026amp; raw, uint16_t r, uint16_t gr, uint16_t gb, uint16_t b, float alpha, float beta, BAYER_PATTERN bayer_pattern, uint16_t clip) { switch (bayer_pattern) { case BAYER_PATTERN_RGGB: for (int y = 0; y \u0026lt; raw.getHeight(); y+=2) { for (int x = 0; x \u0026lt; raw.getWidth(); x+=2) { raw.at(y, x)\t+= r;\t// r raw.at(y + 1, x + 1)\t+= b;\t// b raw.at(y, x + 1)\t+= gr + alpha * raw.at(y, x);\t// gr raw.at(y + 1, x)\t+= gb + beta * raw.at(y + 1, x + 1);\t// gb } } break; case BAYER_PATTERN_BGGR: for (int y = 0; y \u0026lt; raw.getHeight(); y+=2) { for (int x = 0; x \u0026lt; raw.getWidth(); x+=2) { raw.at(y, x) += b; raw.at(y + 1, x + 1) += r; raw.at(y, x + 1) += gb + beta * raw.at(y, x); raw.at(y + 1, x) += gr + alpha * raw.at(y + 1, x + 1); } } break; case BAYER_PATTERN_GBRG: for (int y = 0; y \u0026lt; raw.getHeight(); y+= 2) { for (int x = 0; x \u0026lt; raw.getWidth(); x+= 2) { raw.at(y, x + 1) += b; raw.at(y + 1, x) += r; raw.at(y, x) += gb + beta * raw.at(y, x + 1); raw.at(y + 1, x + 1) += gr + alpha * raw.at(y + 1, x); } } break; case BAYER_PATTERN_GRBG: for (int y = 0; y \u0026lt; raw.getHeight(); y+= 2) { for (int x = 0; x \u0026lt; raw.getWidth(); x+= 2) { raw.at(y + 1, x) += b; raw.at(y, x + 1) += r; raw.at(y + 1, x + 1) += gb + beta * raw.at(y + 1, x);\traw.at(y, x) += gr + alpha * raw.at(y, x + 1); } } break; case BAYER_PATTERN_UNKNOWN: default: TRACE_DEBUG_LOG_ERROR(\u0026#34;Unknown Bayer Pattern:%s\\n\u0026#34;, bayer_pattern); throw \u0026#34;Unknown bayer pattern\u0026#34;; break; } raw.clip(0, clip); } 镜头阴影校正（Lens Shading Correction，LSC） Lens Shading 是相机成像过程中一种由镜头与图像传感器的光学特性引起的现象。具体表现为图像亮度和颜色在视场（Field of View，FOV）中的分布不均匀，通常是图像中心亮度较高，而边缘亮度较低，并可能伴随颜色偏移。它是暗角现象（Vignetting）的扩展，包含了亮度（Luma Shading）和色彩（Chroma Shading）的不均匀性。\n亮度（Luma）阴影 Luma Shading 也被称为渐晕（Vignetting），由于透镜组的光学特性，入射光偏离光轴角度较大时，部分光就会受光阑的影响而无法在感光平面上成像，从而导致越靠近边缘的像素亮度越低。\n此外,对于FSI工艺的sensor，Luma shading的主要成因还包括边缘像素焦点错位，解决这个问题可以通过边缘微透镜的偏移来修正，即从中心像素开始，微透镜的直径都略小于成像面，这样越接近边缘，微透镜与成像面之间的偏移就越大，从而可以补偿入射光线角度过大导致的焦点偏移，使微透镜的CRA增大，从而光线可以更好地汇聚到感光平面上。\n扩展阅读： FSI vs BSI 、 什么是CRA\n色彩（Chroma）阴影 Chroma Shading是Lens Shading的一种特殊情况，主要表现为从中心向边缘的色晕。\nChroma Shading的成因，看了很多博客以及文章，众说纷纭，总结起来主要两个原因：\n对不同波长的光具有不同的折射率，因此在成像时不同色光的焦平面并不重合，从而产生了色差。 由于干涉型红外滤光片（IR-Filter）对不同入射角度的红外光的阻隔效果不同，从而导致部分大角度入射的近红外光没有被阻隔，从而产生了从中心向边缘的色晕。 校正算法 目前常用的镜头阴影校正算法是增益法。由于存储每个像素点的校正系数会占用大量存储空间，实际实现中通常对图像进行n×n 网格划分（一般选择 16×16 的网格），仅存储这些网格点处的校正系数。对于网格之间的其他像素点，其校正系数通过四次余弦插值计算得到。\n四次余弦定律：$I_\\theta = I_0 \\cos^4 \\theta$，即对于一个成像镜头系统，离光轴的像素亮度会随着离轴视场角$\\theta$的增大按$\\cos^4\\theta$的比例下降。这种光学现象是镜头阴影效应的理论基础，校正时需要通过插值计算来补偿亮度。\n而要得到该nxn个格点处的校正系数，在均匀光源下采集参考图像，记录每个像素的实际亮度值，并与理想亮度进行对比，后者与前者的比值即为校正系数。\n$$G(i,j)=\\frac{I_{\\mathrm{ideal}}(i,j)}{I_{\\mathrm{observed}}(i,j)}$$\n示例代码：Luma Shading Correction ✖ 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 /** * @brief Performs lens shading correction (LSC) on RAW image data to compensate * for brightness falloff towards the edges. * * @param raw Reference to the input RAW image data. * @param intensity Intensity of the lens shading correction. Higher values * apply stronger correction. * @param minR Minimum radius (distance from the center) for applying * correction. If less than 0, defaults to 0. * @param maxR Maximum radius (distance from the center) for applying * correction. If less than 0, defaults to the distance from the center to the * farthest corner of the image. * @param clip Maximum allowable pixel value after correction. Values * exceeding this are clipped. */ void LSC(ImageRaw\u0026amp; raw, uint16_t intensity, int minR, int maxR, uint16_t clip) { if (minR \u0026lt; 0) minR = 0; if (maxR \u0026lt; 0) maxR = sqrt(pow(raw.getHeight() / 2, 2) + pow(raw.getWidth() / 2, 2)); for (int y = 0; y \u0026lt; raw.getHeight(); y++) { for (int x = 0; x \u0026lt; raw.getWidth(); x++) { int r = sqrt(pow((y - raw.getHeight() / 2), 2) + pow((x - raw.getWidth() / 2), 2)); float factor = (r - minR) / (maxR - minR); raw.at(y, x) = raw.at(y, x) * (1 + intensity * (factor + 0.5)); } } raw.clip(0, clip); } 示例代码：Chroma Shading Correction ✖ 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 float __cnc(const char* is_color, float center, float avgG, float avgC1, float avgC2, float r_gain, float gr_gain, float gb_gain, float b_gain) { float dampFactor = 1.0; float signalGap = center - MAX(avgG, avgC2); if (strcmp(is_color, \u0026#34;r\u0026#34;) == 0) { if (r_gain \u0026lt;= 1.0) dampFactor = 1.0; else if (r_gain \u0026gt; 1.0 \u0026amp;\u0026amp; r_gain \u0026lt;= 1.2) dampFactor = 0.5; else if (r_gain \u0026gt; 1.2) dampFactor = 0.3; } if (strcmp(is_color, \u0026#34;b\u0026#34;) == 0) { if (b_gain \u0026lt;= 1.0) dampFactor = 1.0; else if (b_gain \u0026gt; 1.0 \u0026amp;\u0026amp; b_gain \u0026lt;= 1.2) dampFactor = 0.5; else if (b_gain \u0026gt; 1.2) dampFactor = 0.3; } float chromaCorrected = MAX(avgG, avgC2) + dampFactor * signalGap; float signalMeter = 0.299 * avgC2 + 0.587 * avgG + 0.144 * avgC1; if (strcmp(is_color, \u0026#34;r\u0026#34;) == 0) signalMeter = 0.299 * avgC1 + 0.587 * avgG + 0.144 * avgC2; else if (strcmp(is_color, \u0026#34;b\u0026#34;) == 0) signalMeter = 0.299 * avgC2 + 0.587 * avgG + 0.144 * avgC1; float fade1 = 0, fade2 = 0; if (signalMeter \u0026lt;= 30) fade1 = 1.0; else if (signalMeter \u0026gt; 30 \u0026amp;\u0026amp; signalMeter \u0026lt;= 50) fade1 = 0.9; else if (signalMeter \u0026gt; 50 \u0026amp;\u0026amp; signalMeter \u0026lt;= 70) fade1 = 0.8; else if (signalMeter \u0026gt; 70 \u0026amp;\u0026amp; signalMeter \u0026lt;= 100) fade1 = 0.7; else if (signalMeter \u0026gt; 100 \u0026amp;\u0026amp; signalMeter \u0026lt;= 150) fade1 = 0.6; else if (signalMeter \u0026gt; 150 \u0026amp;\u0026amp; signalMeter \u0026lt;= 200) fade1 = 0.3; else if (signalMeter \u0026gt; 200 \u0026amp;\u0026amp; signalMeter \u0026lt;= 250) fade1 = 0.1; else fade1 = 0; if (avgC1 \u0026lt;= 30) fade2 = 1.0; else if (avgC1 \u0026gt; 30 \u0026amp;\u0026amp; avgC1 \u0026lt;= 50) fade2 = 0.9; else if (avgC1 \u0026gt; 50 \u0026amp;\u0026amp; avgC1 \u0026lt;= 70) fade2 = 0.8; else if (avgC1 \u0026gt; 70 \u0026amp;\u0026amp; avgC1 \u0026lt;= 100) fade2 = 0.6; else if (avgC1 \u0026gt; 100 \u0026amp;\u0026amp; avgC1 \u0026lt;= 150) fade2 = 0.5; else if (avgC1 \u0026gt; 150 \u0026amp;\u0026amp; avgC1 \u0026lt;= 200) fade2 = 0.3; else fade2 = 0; float fadeTot = fade1 * fade2; return (1 - fadeTot) * center + fadeTot * chromaCorrected; } void __cnd(int y, int x, ImageRaw* img, float thres, int\u0026amp; is_noise, float\u0026amp; avgG, float\u0026amp; avgC1, float\u0026amp; avgC2) { avgG = 0, avgC1 = 0, avgC2 = 0; is_noise = 0; for (int i = y - 4; i \u0026lt; y + 4; i++) { for (int j = x - 4; j \u0026lt; x + 4; j++) { if ((i % 2 == 1) \u0026amp;\u0026amp; (j % 2 == 0))\tavgG = avgG + img-\u0026gt;at(i, j); else if ((i % 2 == 0) \u0026amp;\u0026amp; (j % 2 == 1))\tavgG = avgG + img-\u0026gt;at(i, j); else if ((i % 2 == 0) \u0026amp;\u0026amp; (j % 2 == 0))\tavgC1 = avgC1 + img-\u0026gt;at(i, j); else if ((i % 2 == 1) \u0026amp;\u0026amp; (j % 2 == 1))\tavgC2 = avgC2 + img-\u0026gt;at(i, j); } } avgG = avgG / 40; avgC1 = avgC1 / 25; avgC2 = avgC2 / 16; float center = img-\u0026gt;at(y, x); if ((center \u0026gt; avgG + thres) \u0026amp;\u0026amp; (center \u0026gt; avgC2 + thres)) { if ((avgC1 \u0026gt; avgG + thres) \u0026amp;\u0026amp; (avgC1 \u0026gt; avgC2 + thres)) { is_noise = 1; } else { is_noise = 0; } } else { is_noise = 0; } } float __cnf(const char* is_color, int y, int x, ImageRaw* img, float thres, float r_gain, float gr_gain, float gb_gain, float b_gain) { int is_noise; float avgG, avgC1, avgC2; __cnd(y, x, img, thres, is_noise, avgG, avgC1, avgC2); float pix_out; if (is_noise == 1) { pix_out = __cnc(is_color, img-\u0026gt;at(y, x), avgG, avgC1, avgC2, r_gain, gr_gain, gb_gain, b_gain); } else { pix_out = img-\u0026gt;at(y, x); } return pix_out; } void CNF(ImageRaw\u0026amp; img, BAYER_PATTERN bayer_pattern, float threshold, float r_gain, float gr_gain, float gb_gain, float b_gain, uint16_t clip) { ImageRaw* img_pad = new ImageRaw(img); img_pad-\u0026gt;padding(4, PADDING_MODE_REFLECT); uint16_t r, gr, gb, b; for (int y = 0; y \u0026lt; img_pad-\u0026gt;getHeight() - 8 - 1; y += 2) { for (int x = 0; x \u0026lt; img_pad-\u0026gt;getWidth() - 8 - 1; x += 2) { switch (bayer_pattern) { case BAYER_PATTERN_RGGB: r = img_pad-\u0026gt;at(y + 4, x + 4); gr = img_pad-\u0026gt;at(y + 4, x + 5); gb = img_pad-\u0026gt;at(y + 5, x + 4); b = img_pad-\u0026gt;at(y + 5, x + 5); img.at(y, x) = __cnf(\u0026#34;r\u0026#34;, y + 4, x + 4, img_pad, threshold, r_gain, gr_gain, gb_gain, b_gain); img.at(y, x + 1) = gr; img.at(y + 1, x) = gb; img.at(y + 1, x + 1) = __cnf(\u0026#34;b\u0026#34;, y + 5, x + 5, img_pad, threshold, r_gain, gr_gain, gb_gain, b_gain); break; case BAYER_PATTERN_BGGR: b = img_pad-\u0026gt;at(y + 4, x + 4); gb = img_pad-\u0026gt;at(y + 4, x + 5); gr = img_pad-\u0026gt;at(y + 5, x + 4); r = img_pad-\u0026gt;at(y + 5, x + 5); img.at(y, x) = __cnf(\u0026#34;b\u0026#34;, y + 4, x + 4, img_pad, threshold, r_gain, gr_gain, gb_gain, b_gain); img.at(y, x + 1) = gb; img.at(y + 1, x) = gr; img.at(y + 1, x + 1) = __cnf(\u0026#34;r\u0026#34;, y + 5, x + 5, img_pad, threshold, r_gain, gr_gain, gb_gain, b_gain); break; case BAYER_PATTERN_GBRG: gb = img_pad-\u0026gt;at(y + 4, x + 4); b = img_pad-\u0026gt;at(y + 4, x + 5); r = img_pad-\u0026gt;at(y + 5, x + 4); gr = img_pad-\u0026gt;at(y + 5, x + 5); img.at(y, x) = gb; img.at(y, x + 1) = __cnf(\u0026#34;b\u0026#34;, y + 4, x + 5, img_pad, threshold, r_gain, gr_gain, gb_gain, b_gain); img.at(y + 1, x) = __cnf(\u0026#34;r\u0026#34;, y + 5, x + 4, img_pad, threshold, r_gain, gr_gain, gb_gain, b_gain); img.at(y + 1, x + 1) = gr; break; case BAYER_PATTERN_GRBG: gr = img_pad-\u0026gt;at(y + 4, x + 4); r = img_pad-\u0026gt;at(y + 4, x + 5); b = img_pad-\u0026gt;at(y + 5, x + 4); gb = img_pad-\u0026gt;at(y + 5, x + 5); img.at(y, x) = gr; img.at(y, x + 1) = __cnf(\u0026#34;r\u0026#34;, y + 4, x + 5, img_pad, threshold, r_gain, gr_gain, gb_gain, b_gain); img.at(y + 1, x) = __cnf(\u0026#34;b\u0026#34;, y + 5, x + 4, img_pad, threshold, r_gain, gr_gain, gb_gain, b_gain); img.at(y + 1, x + 1) = gb; break; case BAYER_PATTERN_UNKNOWN: default: TRACE_DEBUG_LOG_ERROR(\u0026#34;Unknown Bayer Pattern:%s\\n\u0026#34;, bayer_pattern); break; } } } img.clip(0, clip); delete img_pad; } 抗锯齿滤波（Anti-aliasing Filtering，AAF） 如果拍摄的图像中含有大量高频成分，在降采样时就会产生锯齿（aliasing），要去除锯齿，需要对原图像进行平滑。常用的方法有高斯滤波器（Gaussian Filter）和双边滤波器（Bilateral Filter）。\n高斯滤波器 $$G(x,y)=\\frac1{2\\pi\\sigma^2}\\exp\\left(-\\frac{x^2+y^2}{2\\sigma^2}\\right)$$\n其中:\n$\\bullet\\quad G(x,y)$是高斯核在坐标(x,y)的值;\n$\\bullet\\quad \\sigma$是高斯分布的标准差,决定滤波器的平滑强度;\n$\\bullet\\quad x,y$是距离核中心的坐标。 双边滤波器 $$W(p,q)=\\exp\\left(-\\frac{\\|p-q\\|^2}{2\\sigma_s^2}\\right)\\cdot\\exp\\left(-\\frac{|I(p)-I(q)|^2}{2\\sigma_r^2}\\right)$$\n其中:\n$\\bullet\\quad W(p,q)$是像素q对p的权重;\n$\\bullet\\quad|p-q|$是两像素在空间上的距离;\n$\\bullet\\quad|I(p)-I(q)|$是两像素灰度值的差异;\n$\\bullet\\quad\\sigma_s$是空间域高斯标准差;\n$\\bullet\\quad\\sigma_r$是强度域高斯标准差。\n滤波结果:\n$$I^{\\prime} (p)=\\frac{\\sum_{q\\in S}W(p,q)\\cdot I(q)}{\\sum_{q\\in S}W(p,q)}$$\n其中:S是滤波窗口,$I^{\\prime}(p)$是像素p的滤波值。 两者对比：\n特性 高斯滤波器 双边滤波器 性质 线性滤波 非线性滤波 边缘保留 不保留边缘，模糊边缘 保留边缘，对边缘影响较小 计算复杂度 低 高 实现难度 简单 较复杂 适用场景 快速平滑、高频噪声较少的场景 高质量抗锯齿，保留纹理和边缘 示例代码（双边滤波器 ✖ 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 /** * @brief Computes the Gaussian weight for a given difference. * * @param x The difference (distance in space or intensity). * @param sigma The standard deviation of the Gaussian function. * @return The computed Gaussian weight. */ double gaussian(double x, double sigma) { return std::exp(-x * x / (2.0 * sigma * sigma)); } /** * @brief Applies a bilateral filter to an ImageRaw object. * * @param src Input image. * @param dst Output image (should be pre-initialized to the same size as src). * @param d The diameter of the filter kernel (must be an odd number). * @param sigmaColor The standard deviation in the intensity space. * @param sigmaSpace The standard deviation in the spatial space. */ void applyBilateralFilter(const ImageRaw\u0026amp; src, ImageRaw\u0026amp; dst, int d, double sigmaColor, double sigmaSpace) { int height = src.getHeight(); int width = src.getWidth(); // Ensure kernel size is odd if (d % 2 == 0) { throw std::invalid_argument(\u0026#34;Kernel size must be odd.\u0026#34;); } // Precompute spatial Gaussian weights int radius = d / 2; std::vector\u0026lt;std::vector\u0026lt;double\u0026gt;\u0026gt; spatialKernel(d, std::vector\u0026lt;double\u0026gt;(d, 0.0)); for (int i = -radius; i \u0026lt;= radius; ++i) { for (int j = -radius; j \u0026lt;= radius; ++j) { spatialKernel[i + radius][j + radius] = gaussian(std::sqrt(i * i + j * j), sigmaSpace); } } // Perform bilateral filtering for (int i = 0; i \u0026lt; height; ++i) { for (int j = 0; j \u0026lt; width; ++j) { double weightedSum = 0.0; double normalizationFactor = 0.0; uint16_t centerPixel = src.at(i, j); for (int ki = -radius; ki \u0026lt;= radius; ++ki) { for (int kj = -radius; kj \u0026lt;= radius; ++kj) { int ni = i + ki; int nj = j + kj; // Skip out-of-bound pixels if (ni \u0026lt; 0 || ni \u0026gt;= height || nj \u0026lt; 0 || nj \u0026gt;= width) { continue; } uint16_t neighborPixel = src.at(ni, nj); // Compute intensity Gaussian weight double intensityWeight = gaussian(std::abs((int)centerPixel - (int)neighborPixel), sigmaColor); // Compute final weight double weight = spatialKernel[ki + radius][kj + radius] * intensityWeight; // Accumulate weighted sum and normalization factor weightedSum += neighborPixel * weight; normalizationFactor += weight; } } // Assign the filtered value to the destination image dst.at(i, j) = static_cast\u0026lt;uint16_t\u0026gt;(weightedSum / normalizationFactor); } } } 自动白平衡（Auto White Balance，AWB） 人的视觉和神经系统具有色彩恒常性，在看到白色物体的时候基本不受环境光源变化的影响。但是image sensor并不能像人的视觉系统一样自动调节，在不同色温光源下，拍出的照片中白色会出现偏色的情况。自动白平衡就是用来模拟人类的色彩恒常能力，在图像中去除光源引起的偏色，从而还原自然的色彩。\n自动白平衡算法根据技术路线可以归结为几大类，分别是：\n场景假设模型：灰度世界、完美反射 点统计模型：白色点估计、分块权重 灰度世界算法\n该算法假设：对于一副有着丰富色彩的图片，图像上R、G、B三个通道的平均值应该等于一个被称为“灰色”的值$K$。至于“灰色”值K的选择，一种方法是将待处理图片三个通道均值的均值作为“灰色”值$K$。当确定了灰色值K之后，待处理图片各个通道的校正系数分别为：$k_r=K/R_{mean}，k_g=K/G_{mean}，k_b=K/B_{mean}$，其中$R_{mean}，G_{mean}和B_{mean}$分别为图像R、G、B通道的均值。 示例代码 ✖ 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 #include \u0026lt;iostream\u0026gt; #include \u0026lt;vector\u0026gt; #include \u0026lt;numeric\u0026gt; #include \u0026lt;stdexcept\u0026gt; /** * @brief Apply the Gray World Algorithm to an ImageRaw. * * This algorithm adjusts the R, G, and B channels of the input image so that their * mean values are equal to a common gray level \\( K \\). This corrects the overall * color balance of the image. * * @param src Input image. Must have 3 channels (RGB format). * @param dst Output image. Must have the same size as the input image. */ void grayWorldAlgorithm(const ImageRaw\u0026amp; src, ImageRaw\u0026amp; dst) { // Validate input image dimensions if (src.getWidth() != dst.getWidth() || src.getHeight() != dst.getHeight()) { throw std::invalid_argument(\u0026#34;Input and output images must have the same dimensions.\u0026#34;); } int width = src.getWidth(); int height = src.getHeight(); // Accumulators for R, G, B channel sums uint64_t sumR = 0, sumG = 0, sumB = 0; int numPixels = width * height; // Compute channel-wise sums for (int i = 0; i \u0026lt; height; ++i) { for (int j = 0; j \u0026lt; width; ++j) { uint16_t r = src.at(i, j * 3); // Assume RGB channels are stored in sequence uint16_t g = src.at(i, j * 3 + 1); uint16_t b = src.at(i, j * 3 + 2); sumR += r; sumG += g; sumB += b; } } // Compute mean values for each channel double meanR = static_cast\u0026lt;double\u0026gt;(sumR) / numPixels; double meanG = static_cast\u0026lt;double\u0026gt;(sumG) / numPixels; double meanB = static_cast\u0026lt;double\u0026gt;(sumB) / numPixels; // Compute the target gray value \\( K \\) as the average of all channel means double K = (meanR + meanG + meanB) / 3.0; // Compute correction factors double kR = K / meanR; double kG = K / meanG; double kB = K / meanB; // Apply correction to each pixel for (int i = 0; i \u0026lt; height; ++i) { for (int j = 0; j \u0026lt; width; ++j) { uint16_t r = src.at(i, j * 3); uint16_t g = src.at(i, j * 3 + 1); uint16_t b = src.at(i, j * 3 + 2); // Adjust each channel and clip the values to the valid range dst.at(i, j * 3) = static_cast\u0026lt;uint16_t\u0026gt;(std::min(std::max(r * kR, 0.0), 65535.0)); dst.at(i, j * 3 + 1) = static_cast\u0026lt;uint16_t\u0026gt;(std::min(std::max(g * kG, 0.0), 65535.0)); dst.at(i, j * 3 + 2) = static_cast\u0026lt;uint16_t\u0026gt;(std::min(std::max(b * kB, 0.0), 65535.0)); } } } 完美反射法 该算法假设：“镜面”可以完全发射光源照射在物体上面的光线。因此，如果图像中存在一个“镜面”的话，那么在特定光源下，可以将所获得的“镜面”的色彩信息认为是当前光源的信息。在进行白平衡校准的时候，假设图片上存在一个可以完全反射光源的“镜面”，那么在经典光源下图片中就应该存在一个三刺激值为[255,255,255]纯白色像素点（有多种白色点定义，这是一种），此时待处理图片各个通道的校正系数分别为：$k_r=255/R_{max}，k_g=255/G_{max}，k_b=255/B_{max}$，其中$R_{max}，G_{max}和B_{max}$分别为图像R、G、B通道的最大值。 示例代码-完美反射法 ✖ 统计加权白点法（通常在需要比较高图像质量时使用） 该算法首先需要对传感器进行光源标定得到色温曲线。折线上的点是在产线上针对不同光源(D65, A光，H光等)使用标准白/灰卡纸拍出照片算出来的$R_{gain}=R_{mean}/G_{mean}$和$B_{gain}=B_{mean}/G_{mean}$坐标。校准时将图片分为M块，分别计算每块的$R_{gain}$和$B_{gain}$和，作为一个“白点”，将值靠近折线区域(红色)的“白点”权重加高，远离的(蓝色)权重降低，再计算出加权平均值得到最终“白点”的$R_{gain}$和$B_{gain}$，使用折线上的不同点做插值计算出一个最终$R_{gain}$和$B_{gain}$值。 示例代码-统计加权白点法 ✖ 去马赛克（Demosaicing） 之前提到了普通的CMOS图像传感器无法“捕获”色彩信息，解决方法是通过Bayer阵列的带有不同颜色滤光片的像素点记录对应通道的亮度值，最后插值得到每个像素点的RGB三通道值。由于通过CMOS得到的Bayer格式的RAW图放大看起来就像马赛克，这个从灰度图片还原图片色彩的过程又叫去马赛克。（Demosaicing）\nDemosiac的插值一般遵循以下几个原则：\n1.先对G分量进行插值，因为G通道的像素数量是GB通道的两倍\n2.插值时采用方向性插值，即如果是垂直的边缘，则采用上下的像素插值，而不选用左右\n3.哈密尔顿(Hamilton)色差恒定原理，$R(i,j)-G(i,j)=R(i,j+1)-G(i,j+1)$，即相邻点色差相同\n4.各个颜色分量在同一像素点处的高频分量可认为是相同的\n具体而言，一个简单的去马赛克流程可以是：\n1.获取图像中的物体的边缘 2.根据边缘信息插值重建G分量 3.根据哈密尔顿提出的色差恒定理论，根据色差插值重建R和B分量 4.一些后处理，包括伪彩色抑制（pseudocolor suppression）和拉链效应抑制（zipper cancelling）等\n扩展阅读：低成本边缘与色差插值去马赛克算法（LED）\n示例代码-LED法 ✖ 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 // 示例代码：去马赛克算法实现（基于方向性插值和色差恒定理论） #include \u0026lt;vector\u0026gt; #include \u0026lt;cmath\u0026gt; #include \u0026lt;algorithm\u0026gt; using namespace std; // 获取梯度信息 void compute_gradients(const vector\u0026lt;vector\u0026lt;int\u0026gt;\u0026gt;\u0026amp; image, vector\u0026lt;vector\u0026lt;int\u0026gt;\u0026gt;\u0026amp; grad_h, vector\u0026lt;vector\u0026lt;int\u0026gt;\u0026gt;\u0026amp; grad_v) { int rows = image.size(); int cols = image[0].size(); grad_h.resize(rows, vector\u0026lt;int\u0026gt;(cols, 0)); grad_v.resize(rows, vector\u0026lt;int\u0026gt;(cols, 0)); for (int i = 1; i \u0026lt; rows - 1; ++i) { for (int j = 1; j \u0026lt; cols - 1; ++j) { grad_h[i][j] = abs(image[i][j + 1] - image[i][j - 1]); grad_v[i][j] = abs(image[i + 1][j] - image[i - 1][j]); } } } // 方向性插值重建G分量 void reconstruct_green(const vector\u0026lt;vector\u0026lt;int\u0026gt;\u0026gt;\u0026amp; bayer, vector\u0026lt;vector\u0026lt;int\u0026gt;\u0026gt;\u0026amp; green, const vector\u0026lt;vector\u0026lt;int\u0026gt;\u0026gt;\u0026amp; grad_h, const vector\u0026lt;vector\u0026lt;int\u0026gt;\u0026gt;\u0026amp; grad_v) { int rows = bayer.size(); int cols = bayer[0].size(); green = bayer; // 初始为 Bayer 图像 for (int i = 1; i \u0026lt; rows - 1; ++i) { for (int j = 1; j \u0026lt; cols - 1; ++j) { if ((i + j) % 2 == 0) { // 当前像素不是 G if (grad_h[i][j] \u0026lt; grad_v[i][j]) { green[i][j] = (bayer[i][j - 1] + bayer[i][j + 1]) / 2; } else { green[i][j] = (bayer[i - 1][j] + bayer[i + 1][j]) / 2; } } } } } // 根据色差恒定理论重建 R 和 B 分量 void reconstruct_red_blue(const vector\u0026lt;vector\u0026lt;int\u0026gt;\u0026gt;\u0026amp; bayer, const vector\u0026lt;vector\u0026lt;int\u0026gt;\u0026gt;\u0026amp; green, vector\u0026lt;vector\u0026lt;int\u0026gt;\u0026gt;\u0026amp; red, vector\u0026lt;vector\u0026lt;int\u0026gt;\u0026gt;\u0026amp; blue) { int rows = bayer.size(); int cols = bayer[0].size(); red = bayer; // 初始为 Bayer 图像 blue = bayer; for (int i = 1; i \u0026lt; rows - 1; ++i) { for (int j = 1; j \u0026lt; cols - 1; ++j) { if ((i % 2 == 0 \u0026amp;\u0026amp; j % 2 == 1) || (i % 2 == 1 \u0026amp;\u0026amp; j % 2 == 0)) { // 当前像素是 G red[i][j] = green[i][j] + (bayer[i][j + 1] - green[i][j + 1] + bayer[i][j - 1] - green[i][j - 1]) / 2; blue[i][j] = green[i][j] + (bayer[i + 1][j] - green[i + 1][j] + bayer[i - 1][j] - green[i - 1][j]) / 2; } else if ((i % 2 == 0 \u0026amp;\u0026amp; j % 2 == 0)) { // 当前像素是 B red[i][j] = green[i][j] + (bayer[i + 1][j + 1] - green[i + 1][j + 1] + bayer[i - 1][j - 1] - green[i - 1][j - 1]) / 2; } else { // 当前像素是 R blue[i][j] = green[i][j] + (bayer[i + 1][j + 1] - green[i + 1][j + 1] + bayer[i - 1][j - 1] - green[i - 1][j - 1]) / 2; } } } } // 伪彩色抑制与拉链效应去除的后处理 void post_process(vector\u0026lt;vector\u0026lt;int\u0026gt;\u0026gt;\u0026amp; red, vector\u0026lt;vector\u0026lt;int\u0026gt;\u0026gt;\u0026amp; green, vector\u0026lt;vector\u0026lt;int\u0026gt;\u0026gt;\u0026amp; blue) { int rows = red.size(); int cols = red[0].size(); for (int i = 1; i \u0026lt; rows - 1; ++i) { for (int j = 1; j \u0026lt; cols - 1; ++j) { red[i][j] = min(max(red[i][j], 0), 255); green[i][j] = min(max(green[i][j], 0), 255); blue[i][j] = min(max(blue[i][j], 0), 255); } } } // 主函数：去马赛克 void demosaic(const vector\u0026lt;vector\u0026lt;int\u0026gt;\u0026gt;\u0026amp; bayer, vector\u0026lt;vector\u0026lt;int\u0026gt;\u0026gt;\u0026amp; red, vector\u0026lt;vector\u0026lt;int\u0026gt;\u0026gt;\u0026amp; green, vector\u0026lt;vector\u0026lt;int\u0026gt;\u0026gt;\u0026amp; blue) { vector\u0026lt;vector\u0026lt;int\u0026gt;\u0026gt; grad_h, grad_v; compute_gradients(bayer, grad_h, grad_v); // 计算梯度 reconstruct_green(bayer, green, grad_h, grad_v); // 重建 G 分量 reconstruct_red_blue(bayer, green, red, blue); // 重建 R 和 B 分量 post_process(red, green, blue); // 后处理 } 色彩校正（Color Correction） $$\\begin{bmatrix}R'\\\\G'\\\\B'\\end{bmatrix}=\\begin{bmatrix}m_{11}\u0026m_{12}\u0026m_{13}\\\\m_{21}\u0026m_{22}\u0026m_{23}\\\\m_{31}\u0026m_{32}\u0026m_{33}\\end{bmatrix}\\begin{bmatrix}R\\\\G\\\\B\\end{bmatrix}$$ 其中R\u0026rsquo;G\u0026rsquo;B\u0026rsquo;是校正后的颜色值，RGB是传感器捕获的原始颜色值。\n示例代码 ✖ 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 #include \u0026lt;vector\u0026gt; #include \u0026lt;array\u0026gt; // 色彩校正矩阵定义 const std::array\u0026lt;std::array\u0026lt;float, 3\u0026gt;, 3\u0026gt; CCM = {{ {1.2f, -0.1f, -0.1f}, {-0.2f, 1.3f, -0.1f}, {-0.1f, -0.2f, 1.4f} }}; // 对单个像素点进行色彩校正 std::array\u0026lt;float, 3\u0026gt; colorCorrection(const std::array\u0026lt;float, 3\u0026gt;\u0026amp; input) { std::array\u0026lt;float, 3\u0026gt; output = {0.0f, 0.0f, 0.0f}; for (int i = 0; i \u0026lt; 3; ++i) { for (int j = 0; j \u0026lt; 3; ++j) { output[i] += CCM[i][j] * input[j]; } } return output; } 色彩校正后，也可以根据需求对校正后的颜色进一步调节,例如色彩增强或风格化\n伽马矫正（Gamma Correction） 人类对亮度的感知并非线性，而是接近幂函数$L\\propto I^{0.43}$。这意味着我们对暗部细节更加敏感，对亮部变化的感知能力较弱。为了高效利用有限的位深（bit数），优化编码效率，图像编码会按照人类的视觉特性进行非线性变换，将更多的编码精度分配到暗部（人类更敏感的部分），减少亮部（人类较不敏感部分）的精度。这种非线性编码通常被称为gamma编码。基于这种编码特性，显示设备（如显示器）为了使图像在视觉上看起来正确，会执行“反Gamma”操作$I_{output}=I_{input}^{\\frac1\\gamma}，\\gamma$常取2.2，即将编码图像的非线性亮度还原为线性亮度。\n为什么需要Gamma编码：\n如果直接对图像进行线性编码并存储或传输，暗部细节会被过度压缩，造成重要的视觉信息丢失。同时，显示设备的反Gamma校正会进一步放大这种现象，使得图像暗部显示更差。因此，为了保证在显示设备上保持预期的亮度和对比度，拍摄的图像需要在编码阶段进行Gamma编码。\n示例代码 ✖ 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 #include \u0026lt;cmath\u0026gt; // 伽马矫正函数 float gammaCorrection(float value, float gamma) { return std::pow(value, gamma); } // 对整幅图像进行伽马矫正 void processImageGammaCorrection(std::vector\u0026lt;std::array\u0026lt;float, 3\u0026gt;\u0026gt;\u0026amp; image, float gamma) { for (auto\u0026amp; pixel : image) { for (int i = 0; i \u0026lt; 3; ++i) { pixel[i] = gammaCorrection(pixel[i], gamma); } } } 其他图像处理算法 直方图均衡化（Histogram Equalization） 直方图均衡化是一种图像处理技术，通过调整图像的灰度级分布，使得图像的对比度得到改善，但如果对高对比度图像使用时可能导致细节丢失或噪声增强。直方图均衡化更适用于曝光不足、对比度低的图像，如医学影像、卫星图像或计算机视觉中的图像预处理。\n示例代码 ✖ 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 #include \u0026lt;vector\u0026gt; #include \u0026lt;algorithm\u0026gt; #include \u0026lt;numeric\u0026gt; // 计算累积分布函数（CDF） std::vector\u0026lt;float\u0026gt; calculateCDF(const std::vector\u0026lt;int\u0026gt;\u0026amp; histogram) { std::vector\u0026lt;float\u0026gt; cdf(histogram.size(), 0.0f); std::partial_sum(histogram.begin(), histogram.end(), cdf.begin()); float maxValue = cdf.back(); for (float\u0026amp; val : cdf) { val /= maxValue; } return cdf; } // 应用直方图均衡化 void histogramEqualization(std::vector\u0026lt;std::vector\u0026lt;int\u0026gt;\u0026gt;\u0026amp; image, int maxValue) { std::vector\u0026lt;int\u0026gt; histogram(maxValue + 1, 0); // 统计直方图 for (const auto\u0026amp; row : image) { for (int value : row) { ++histogram[value]; } } // 计算CDF std::vector\u0026lt;float\u0026gt; cdf = calculateCDF(histogram); // 应用均衡化 for (auto\u0026amp; row : image) { for (int\u0026amp; value : row) { value = static_cast\u0026lt;int\u0026gt;(cdf[value] * maxValue); } } } 颜色空间转换（Color Space Conversion） ■ RGB(Red, Green, Blue)是一种基于加色模型的颜色空间，表示红色、绿色和蓝色三个颜色通道的强度。RGB模型广泛用于显示设备（如计算机显示器、电视等），它通过不同强度的红、绿、蓝光的组合来生成各种颜色。颜色的混合方式是加法的，即所有通道的光强度加起来会使颜色变得更亮。\n■ CMYK(Cyan, Magenta, Yellow, Black)表示印刷上用的四种颜色，是RGB的补色。由于在实际应用中，青色、洋红色和黄色很难叠加形成真正的黑色，因此引入了K——黑色。黑色的作用是强化暗调，加深暗部色彩。\n■ YUV(Y, U, V)是一种基于亮度和色度分量的颜色空间，通常用于视频压缩和广播中。在YUV模型中，Y表示亮度分量（即灰度信息），U和V表示色度分量（即颜色信息，U表示蓝色投影，V表示红色投影）。YUV的好处在于，它能够将亮度和色度分开处理，这对于图像和视频压缩很有帮助，因为人眼对亮度的敏感度远高于色度，因此可以对色度分量进行较高的压缩而不显著影响视觉质量。\nRGB-YUV 转换公式 ✖ $$\\begin{aligned} \u0026Y=W_RR^{\\prime}+W_GG^{\\prime}+W_BB^{\\prime}=0.299R^{\\prime}+0.587G^{\\prime}+0.114B^{\\prime} \\\\ \u0026U=U_{\\mathrm{max}}\\frac{B^{\\prime}-Y^{\\prime}}{1-W_{B}}\\approx0.492(B^{\\prime}-Y^{\\prime}) \\\\ \u0026V=V_{\\mathrm{max}}\\frac{R^{\\prime}-Y^{\\prime}}{1-W_{R}}\\approx0.877(R^{\\prime}-Y^{\\prime}) \\\\ \u0026\\Rightarrow \\begin{bmatrix}Y\\\\U\\\\V\\end{bmatrix}=\\begin{bmatrix}0.299\u00260.587\u00260.114\\\\-0.14713\u0026-0.28886\u00260.436\\\\0.615\u0026-0.51499\u0026-0.10001\\end{bmatrix}\\begin{bmatrix}R'\\\\G'\\\\B'\\end{bmatrix} \\\\ \u0026R^{\\prime}=Y^{\\prime}+V\\frac{1-W_{R}}{V_{\\mathrm{max}}}=Y^{\\prime}+\\frac{V}{0.877}=Y^{\\prime}+1.14V \\\\ \u0026B^{\\prime}=Y^{\\prime}+U\\frac{1-W_{B}}{U_{\\max}}=Y^{\\prime}+\\frac{U}{0.492}=Y^{\\prime}+2.033U \\\\ \u0026G^{\\prime}=\\frac{Y'-W_RR'-W_BB'}{W_G} \\\\ \u0026\\Rightarrow \\begin{bmatrix}R'\\\\G'\\\\B'\\end{bmatrix}=\\begin{bmatrix}1\u00260\u00261.13983\\\\1\u0026-0.39465\u0026-0.58060\\\\1\u00262.03211\u00260\\end{bmatrix}\\begin{bmatrix}Y\\\\U\\\\V\\end{bmatrix} \\end{aligned}$$ ■ HSV(Hue, Saturation, Value)是根据颜色的直观特性由A. R. Smith在1978年创建的一种颜色空间, 也称六角锥体模型(Hexcone Model)。这个模型中颜色的参数分别是：色调（H），饱和度（S），明度（V）。\n■ HSL(Hue, Saturation, Lightness)是HSV模型的变体，也是由色调、饱和度和亮度三个参数来描述颜色的。与HSV不同，HSL中的亮度（Lightness）指的是从黑色到白色的亮度程度，位于0到100之间，而HSV中的明度（Value）则是从黑色到最亮的颜色的亮度程度。HSL模型常用于计算机图形和图像处理领域，因为它与人眼对颜色的感知更为接近。\nRGB到HSV和HSL的转换比较复杂，参考wiki百科HSL_and_HSV\n示例代码 ✖ 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 // RGB 转 YUV std::array\u0026lt;float, 3\u0026gt; rgbToYuv(const std::array\u0026lt;float, 3\u0026gt;\u0026amp; rgb) { float y = 0.299f * rgb[0] + 0.587f * rgb[1] + 0.114f * rgb[2]; float u = 0.492f * (rgb[2] - y); float v = 0.877f * (rgb[0] - y); return {y, u, v}; } // YUV 转 RGB std::array\u0026lt;float, 3\u0026gt; yuvToRgb(const std::array\u0026lt;float, 3\u0026gt;\u0026amp; yuv) { float r = yuv[0] + 1.14f * yuv[2]; float g = yuv[0] - 0.394f * yuv[1] - 0.581f * yuv[2]; float b = yuv[0] + 2.032f * yuv[1]; return {r, g, b}; } Refference：\nhttps://www.qinxing.xyz/posts/506138d8/\nhttps://github.com/yuqing-liu-dut/ISPLab\nhttps://www.bilibili.com/video/BV1LA4m1N78h\nhttps://www.wpgdadatong.com.cn/blog/detail/42592\nhttps://www.cnblogs.com/lanlancky/p/17498055.html#_label2_0\nhttps://www.cnblogs.com/wnwin/p/11985194.html\nhttps://www.cnblogs.com/sunny-li/p/8641767.html\nhttps://blog.csdn.net/hhy321/article/details/120896015\n","date":"2024-11-11T10:35:55.591Z","image":"https://RoboticsChen.github.io/articles/camera-isp/raw_image_hu_72fbad4f17692848.png","permalink":"https://RoboticsChen.github.io/articles/camera-isp/","title":"相机成像与ISP"}]